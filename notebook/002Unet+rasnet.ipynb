{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/y2023/m2311203/.conda/envs/ultralytics-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGGING TIME OF START: 2025-01-05 20:07:21\n",
      "TRAIN ID: ['TS_69_2', 'TS_6_4', 'TS_6_6', 'TS_86_3']\n",
      "MODE: local\n",
      "SETTING OK!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import Net,run_check_net\n",
    "from dataset import read_one_data, read_one_truth\n",
    "from czii_helper import dotdict\n",
    "\n",
    "#分子の種類\n",
    "MOLECULES = ['apo-ferritin', 'beta-amylase', 'beta-galactosidase', 'ribosome', 'thyroglobulin', 'virus-like-particle']\n",
    "value_map = {m: i+1 for i, m in enumerate(MOLECULES)}\n",
    "\n",
    "# ログ出力\n",
    "print('LOGGING TIME OF START:', datetime.strftime(datetime.now(pytz.timezone('Asia/Singapore')), \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# データセットディレクトリ\n",
    "DATA_KAGGLE_DIR = '../input/czii-cryo-et-object-identification'\n",
    "\n",
    "# モード設定\n",
    "MODE = 'local'\n",
    "if MODE == 'local':\n",
    "    valid_dir = f'{DATA_KAGGLE_DIR}/train'\n",
    "    valid_id = ['TS_5_4', 'TS_73_6', 'TS_99_9']\n",
    "else:\n",
    "    valid_dir = f'{DATA_KAGGLE_DIR}/test'\n",
    "    valid_id = [os.path.basename(path) for path in os.listdir(f'{valid_dir}/static/ExperimentRuns')]\n",
    "\n",
    "# 学習ディレクトリ\n",
    "\n",
    "train_dir = f'{DATA_KAGGLE_DIR}/train'\n",
    "train_id = [os.path.basename(path) for path in os.listdir(f'{train_dir}/static/ExperimentRuns')]\n",
    "train_id = [id for id in train_id if id not in valid_id]\n",
    "print('TRAIN ID:', train_id)\n",
    "\n",
    "# モデル設定\n",
    "cfg = dotdict({\n",
    "    'arch': 'resnet34d',\n",
    "    'threshold': {\n",
    "        'apo-ferritin': 0.05,\n",
    "        'beta-amylase': 0.05,\n",
    "        'beta-galactosidase': 0.05,\n",
    "        'ribosome': 0.05,\n",
    "        'thyroglobulin': 0.05,\n",
    "        'virus-like-particle': 0.05,\n",
    "    },\n",
    "})\n",
    "print('MODE:', MODE)\n",
    "print('SETTING OK!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "\n",
    "# マスクデータ生成（仮データ例として利用可能）\n",
    "num_slice = 100\n",
    "mask = np.zeros((num_slice, 640, 640), dtype=np.float32)\n",
    "\n",
    "# 仮データの設定\n",
    "mask[50, 200:220, 300:320] = 1  # 値1のボクセル\n",
    "mask[70, 100:120, 400:420] = 2  # 値2のボクセル\n",
    "mask[30, 300:320, 200:220] = 3  # 値3のボクセル\n",
    "\n",
    "### 1. スライスごとの2D可視化 ###\n",
    "def visualize_slices(mask, slice_indices):\n",
    "    \"\"\"\n",
    "    マスクの特定のスライスを可視化\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, len(slice_indices) * 4))\n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        plt.subplot(1, len(slice_indices), i + 1)\n",
    "        plt.imshow(mask[slice_idx], cmap='viridis', origin='lower')\n",
    "        plt.colorbar(label=\"Values\")\n",
    "        plt.title(f\"Slice {slice_idx}\")\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "### 2. 3D可視化 ###\n",
    "def visualize_3d(mask):\n",
    "    \"\"\"\n",
    "    マスクの3D可視化\n",
    "    \"\"\"\n",
    "    # ボクセルの座標を取得\n",
    "    z, y, x = np.nonzero(mask)\n",
    "\n",
    "    # 色の設定（値に基づいて色を変える）\n",
    "    colors = np.zeros((len(z), 4))  # RGBA\n",
    "    for i, value in enumerate(mask[z, y, x]):\n",
    "        if value == 1:  # 値1は赤\n",
    "            colors[i] = [1, 0, 0, 0.8]\n",
    "        elif value == 2:  # 値2は緑\n",
    "            colors[i] = [0, 1, 0, 0.8]\n",
    "        elif value == 3:  # 値3は青\n",
    "            colors[i] = [0, 0, 1, 0.8]\n",
    "\n",
    "    # プロット\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(x, y, z, c=colors, marker='o', s=10)\n",
    "\n",
    "    # 軸ラベル\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    ax.set_zlabel(\"Z\")\n",
    "    ax.set_title(\"3D Visualization of Mask\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_volume(mask_dict,z_min,num_slice):\n",
    "    result = {}\n",
    "    for key, array in mask_dict.items():\n",
    "        # zが範囲内にある行を抽出\n",
    "        filtered = array[(array[:, 2] >= z_min*10) & (array[:, 2] <= (z_min*10)+num_slice* 10)]\n",
    "        if filtered.size > 0:  # 抽出結果が空でない場合のみ\n",
    "            result[key] = filtered\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 TS_69_2 ---------------\n",
      "184 630 630\n",
      "1 TS_6_4 ---------------\n",
      "184 630 630\n",
      "2 TS_6_6 ---------------\n",
      "184 630 630\n",
      "3 TS_86_3 ---------------\n",
      "184 630 630\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "slice_id = 0\n",
    "num_slice = 32\n",
    "D = 184\n",
    "num_depth = len(list(range(0, D - num_slice, num_slice//2)) + [D - num_slice]) \n",
    "\n",
    "slice_volume = np.zeros((len(train_id)*num_depth, num_slice, 640, 640), dtype=np.float32)\n",
    "mask_volume = np.zeros((len(train_id)*num_depth, num_slice, 640, 640), dtype=np.float32)\n",
    "\n",
    "for i,id in enumerate(train_id):\n",
    "    print(i, id, '---------------')\n",
    "    volume = read_one_data(id, static_dir=f'{valid_dir}/static/ExperimentRuns')\n",
    "    mask_dict = read_one_truth(id, f'{valid_dir}/overlay/ExperimentRuns')\n",
    "    D, H, W = volume.shape\n",
    "    print(D, H, W)\n",
    "\n",
    "    pad_volume = np.pad(volume, [[0, 0], [0, 640 - H], [0, 640 - W]], mode='constant', constant_values=0)\n",
    "\n",
    "    zz = list(range(0, D - num_slice, num_slice//2)) + [D - num_slice]\n",
    "    for z in zz:\n",
    "        slice_volume[slice_id] = pad_volume[z:z+num_slice]\n",
    "        data = cutout_volume(mask_dict, z, num_slice)\n",
    "        mask = np.zeros((num_slice, 640, 640), dtype=np.float32)\n",
    "        for key, points in data.items():\n",
    "            value = value_map[key]\n",
    "            for point in points:\n",
    "                x, y, z = point\n",
    "                z = z - 640\n",
    "                # インデックスに変換（スケールダウン）\n",
    "                x_idx = int(x * 0.1)\n",
    "                y_idx = int(y * 0.1)\n",
    "                z_idx = int(z * 0.1)\n",
    "                # インデックスがマスクの範囲内であれば値を設定\n",
    "                if 0 <= x_idx < mask.shape[2] and 0 <= y_idx < mask.shape[1] and 0 <= z_idx < mask.shape[0]:\n",
    "                    mask[z_idx, y_idx, x_idx] = value\n",
    "        mask_volume[slice_id] = mask\n",
    "        if np.sum(mask) > 0:\n",
    "            slice_id += 1\n",
    "\n",
    "print(slice_id)\n",
    "\n",
    "slice_volume = slice_volume[:slice_id]\n",
    "mask_volume = mask_volume[:slice_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7eb51cecffd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 48\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle\u001b[39m\u001b[38;5;124m'\u001b[39m], masks)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ultralytics-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ultralytics-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/notebook/kaggle-CZII /notebook/model.py:130\u001b[0m, in \u001b[0;36mNet.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    127\u001b[0m output \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m#<todo> weighted cross entropy\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask_loss\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logit, \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_type:\n\u001b[1;32m    133\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logit,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mask'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import Net  # Assuming \"Net\" is defined in model.py\n",
    "\n",
    "# Example dataset class (replace with your actual dataset logic)\n",
    "class ExampleDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'image': self.data[idx],\n",
    "            'mask': self.labels[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = ExampleDataset(slice_volume, mask_volume)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model Initialization\n",
    "model = Net(pretrained=True, cfg=None).cuda()\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training Loop\n",
    "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            # Move data to GPU\n",
    "            images = batch['image'].cuda()\n",
    "            masks = batch['mask'].cuda()\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model({'image': images})\n",
    "            loss = criterion(outputs['particle'], masks)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
