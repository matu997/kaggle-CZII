{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n",
    "import zarr\n",
    "from scipy.spatial import cKDTree\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from czii_helper import *\n",
    "from dataset import *\n",
    "from model2 import *\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import sys\n",
    "from typing import List, Tuple, Union\n",
    "import zarr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_tomogram is deprecated, use get_tomograms instead. Results may be incomplete\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 394.63it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.40it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 366.41it/s]\n",
      "100%|██████████| 98/98 [00:04<00:00, 19.74it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 352.36it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.11it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 374.99it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.11it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 364.47it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.51it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 401.14it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.58it/s]\n",
      "Loading dataset: 100%|██████████| 98/98 [00:00<00:00, 354.63it/s]\n",
      "100%|██████████| 98/98 [00:05<00:00, 19.57it/s]\n"
     ]
    }
   ],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        spatial_dims: int = 3,\n",
    "        in_channels: int = 1,\n",
    "        out_channels: int = 7,\n",
    "        channels: Union[Tuple[int, ...], List[int]] = (48, 64, 80, 80),\n",
    "        strides: Union[Tuple[int, ...], List[int]] = (2, 2, 1),\n",
    "        num_res_units: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = UNet(\n",
    "            spatial_dims=self.hparams.spatial_dims,\n",
    "            in_channels=self.hparams.in_channels,\n",
    "            out_channels=self.hparams.out_channels,\n",
    "            channels=self.hparams.channels,\n",
    "            strides=self.hparams.strides,\n",
    "            num_res_units=self.hparams.num_res_units,\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "channels = (48, 64, 80, 80)\n",
    "strides_pattern = (2, 2, 1)\n",
    "num_res_units = 1\n",
    "def extract_3d_patches_minimal_overlap(arrays: List[np.ndarray], patch_size: int) -> Tuple[List[np.ndarray], List[Tuple[int, int, int]]]:\n",
    "    if not arrays or not isinstance(arrays, list):\n",
    "        raise ValueError(\"Input must be a non-empty list of arrays\")\n",
    "    \n",
    "    # Verify all arrays have the same shape\n",
    "    shape = arrays[0].shape\n",
    "    if not all(arr.shape == shape for arr in arrays):\n",
    "        raise ValueError(\"All input arrays must have the same shape\")\n",
    "    \n",
    "    if patch_size > min(shape):\n",
    "        raise ValueError(f\"patch_size ({patch_size}) must be smaller than smallest dimension {min(shape)}\")\n",
    "    \n",
    "    m, n, l = shape\n",
    "    patches = []\n",
    "    coordinates = []\n",
    "    \n",
    "    # Calculate starting positions for each dimension\n",
    "    x_starts = calculate_patch_starts(m, patch_size)\n",
    "    y_starts = calculate_patch_starts(n, patch_size)\n",
    "    z_starts = calculate_patch_starts(l, patch_size)\n",
    "    \n",
    "    # Extract patches from each array\n",
    "    for arr in arrays:\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                for z in z_starts:\n",
    "                    patch = arr[\n",
    "                        x:x + patch_size,\n",
    "                        y:y + patch_size,\n",
    "                        z:z + patch_size\n",
    "                    ]\n",
    "                    patches.append(patch)\n",
    "                    coordinates.append((x, y, z))\n",
    "    \n",
    "    return patches, coordinates\n",
    "def reconstruct_array(patches: List[np.ndarray], \n",
    "                     coordinates: List[Tuple[int, int, int]], \n",
    "                     original_shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    reconstructed = np.zeros(original_shape, dtype=np.int64)  # To track overlapping regions\n",
    "    \n",
    "    patch_size = patches[0].shape[0]\n",
    "    \n",
    "    for patch, (x, y, z) in zip(patches, coordinates):\n",
    "        reconstructed[\n",
    "            x:x + patch_size,\n",
    "            y:y + patch_size,\n",
    "            z:z + patch_size\n",
    "        ] = patch\n",
    "        \n",
    "    \n",
    "    return reconstructed\n",
    "def calculate_patch_starts(dimension_size: int, patch_size: int) -> List[int]:\n",
    "    if dimension_size <= patch_size:\n",
    "        return [0]\n",
    "        \n",
    "    # Calculate number of patches needed\n",
    "    n_patches = np.ceil(dimension_size / patch_size)\n",
    "    \n",
    "    if n_patches == 1:\n",
    "        return [0]\n",
    "    \n",
    "    # Calculate overlap\n",
    "    total_overlap = (n_patches * patch_size - dimension_size) / (n_patches - 1)\n",
    "    \n",
    "    # Generate starting positions\n",
    "    positions = []\n",
    "    for i in range(int(n_patches)):\n",
    "        pos = int(i * (patch_size - total_overlap))\n",
    "        if pos + patch_size > dimension_size:\n",
    "            pos = dimension_size - patch_size\n",
    "        if pos not in positions:  # Avoid duplicates\n",
    "            positions.append(pos)\n",
    "    \n",
    "    return positions\n",
    "import pandas as pd\n",
    "\n",
    "def dict_to_df(coord_dict, experiment_name):\n",
    "    # Create lists to store data\n",
    "    all_coords = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Process each label and its coordinates\n",
    "    for label, coords in coord_dict.items():\n",
    "        all_coords.append(coords)\n",
    "        all_labels.extend([label] * len(coords))\n",
    "    \n",
    "    # Concatenate all coordinates\n",
    "    all_coords = np.vstack(all_coords)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'experiment': experiment_name,\n",
    "        'particle_type': all_labels,\n",
    "        'x': all_coords[:, 0],\n",
    "        'y': all_coords[:, 1],\n",
    "        'z': all_coords[:, 2]\n",
    "    })\n",
    "\n",
    "    \n",
    "    return df\n",
    "from typing import List, Tuple, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.data import DataLoader, Dataset, CacheDataset, decollate_batch\n",
    "from monai.transforms import (\n",
    "    Compose, \n",
    "    EnsureChannelFirstd, \n",
    "    Orientationd,  \n",
    "    AsDiscrete,  \n",
    "    RandFlipd, \n",
    "    RandRotate90d, \n",
    "    NormalizeIntensityd,\n",
    "    RandCropByLabelClassesd,\n",
    ")\n",
    "TRAIN_DATA_DIR = \"../input/mask\"\n",
    "import json\n",
    "copick_config_path = TRAIN_DATA_DIR + \"/copick.config\"\n",
    "\n",
    "with open(copick_config_path) as f:\n",
    "    copick_config = json.load(f)\n",
    "\n",
    "copick_config['static_root'] = '../input/czii-cryo-et-object-identification/train/static'\n",
    "\n",
    "copick_test_config_path = 'copick_test.config'\n",
    "\n",
    "with open(copick_test_config_path, 'w') as outfile:\n",
    "    json.dump(copick_config, outfile)\n",
    "import copick\n",
    "\n",
    "root = copick.from_file(copick_test_config_path)\n",
    "\n",
    "copick_user_name = \"copickUtils\"\n",
    "copick_segmentation_name = \"paintedPicks\"\n",
    "voxel_size = 10\n",
    "tomo_type = \"denoised\"\n",
    "inference_transforms = Compose([\n",
    "    EnsureChannelFirstd(keys=[\"image\"], channel_dim=\"no_channel\"),\n",
    "    NormalizeIntensityd(keys=\"image\"),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\")\n",
    "])\n",
    "import cc3d\n",
    "\n",
    "id_to_name = {1: \"apo-ferritin\", \n",
    "              2: \"beta-amylase\",\n",
    "              3: \"beta-galactosidase\", \n",
    "              4: \"ribosome\", \n",
    "              5: \"thyroglobulin\", \n",
    "              6: \"virus-like-particle\"}\n",
    "BLOB_THRESHOLD = 200\n",
    "CERTAINTY_THRESHOLD = 0.05\n",
    "\n",
    "classes = [1, 2, 3, 4, 5, 6]\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cc3d\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import Compose, EnsureType\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from monai.networks.nets import UNet\n",
    "from monai.losses import TverskyLoss\n",
    "from monai.metrics import DiceMetric\n",
    "\n",
    "def load_models(model_paths):\n",
    "    models = []\n",
    "    for model_path in model_paths:\n",
    "        channels = (48, 64, 80, 80)\n",
    "        strides_pattern = (2, 2, 1)       \n",
    "        num_res_units = 1\n",
    "        learning_rate = 1e-3\n",
    "        num_epochs = 100\n",
    "        model = Model(channels=channels, strides=strides_pattern, num_res_units=num_res_units)\n",
    "        \n",
    "        weights =torch.load(model_path)['state_dict']\n",
    "        model.load_state_dict(weights)\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    #'../model/UNet-Model-val_metric0.450.ckpt',\n",
    "    '../model/model_3dunet.ckpt',\n",
    "]\n",
    "\n",
    "\n",
    "models = load_models(model_paths)\n",
    "def ensemble_prediction_tta(models, input_tensor, threshold=0.5):\n",
    "    probs_list = []\n",
    "    data_copy0 = input_tensor.clone()\n",
    "    data_copy0=torch.flip(data_copy0, dims=[2])\n",
    "    data_copy1 = input_tensor.clone()\n",
    "    data_copy1=torch.flip(data_copy1, dims=[3])\n",
    "    data_copy2 = input_tensor.clone()\n",
    "    data_copy2=torch.flip(data_copy2, dims=[4])\n",
    "    data_copy3 = input_tensor.clone()\n",
    "    data_copy3 = data_copy3.rot90(1, dims=[3, 4])\n",
    "    with torch.no_grad():\n",
    "        model_output0 = model(input_tensor)\n",
    "        model_output1 = model(data_copy0)\n",
    "        model_output1=torch.flip(model_output1, dims=[2])\n",
    "        model_output2 = model(data_copy1)\n",
    "        model_output2=torch.flip(model_output2, dims=[3])\n",
    "        model_output3 = model(data_copy2)\n",
    "        model_output3=torch.flip(model_output3, dims=[4])\n",
    "        probs0 = torch.softmax(model_output0[0], dim=0)\n",
    "        probs1 = torch.softmax(model_output1[0], dim=0)\n",
    "        probs2 = torch.softmax(model_output2[0], dim=0)\n",
    "        probs3 = torch.softmax(model_output3[0], dim=0)\n",
    "        probs_list.append(probs0)\n",
    "        probs_list.append(probs1)\n",
    "        probs_list.append(probs2)\n",
    "        probs_list.append(probs3)\n",
    "    avg_probs = torch.mean(torch.stack(probs_list), dim=0)\n",
    "    thresh_probs = avg_probs > threshold\n",
    "    _, max_classes = thresh_probs.max(dim=0)\n",
    "    return max_classes\n",
    "sub=[]\n",
    "for model in models:\n",
    "    with torch.no_grad():\n",
    "        location_df = []\n",
    "        for run in root.runs:\n",
    "            tomo = run.get_voxel_spacing(10)\n",
    "            tomo = tomo.get_tomogram(tomo_type).numpy()\n",
    "            tomo_patches, coordinates = extract_3d_patches_minimal_overlap([tomo], 96)\n",
    "            tomo_patched_data = [{\"image\": img} for img in tomo_patches]\n",
    "            tomo_ds = CacheDataset(data=tomo_patched_data, transform=inference_transforms, cache_rate=1.0)\n",
    "            pred_masks = []\n",
    "            for i in tqdm(range(len(tomo_ds))):\n",
    "                input_tensor = tomo_ds[i]['image'].unsqueeze(0).to(\"cuda\")\n",
    "                max_classes = ensemble_prediction_tta(models, input_tensor, threshold=CERTAINTY_THRESHOLD)\n",
    "                pred_masks.append(max_classes.cpu().numpy())\n",
    "            reconstructed_mask = reconstruct_array(pred_masks, coordinates, tomo.shape)\n",
    "            location = {}\n",
    "            for c in classes:\n",
    "                cc = cc3d.connected_components(reconstructed_mask == c)\n",
    "                stats = cc3d.statistics(cc)\n",
    "                zyx = stats['centroids'][1:] * 10.012444  # 转换单位\n",
    "                zyx_large = zyx[stats['voxel_counts'][1:] > BLOB_THRESHOLD]\n",
    "                xyz = np.ascontiguousarray(zyx_large[:, ::-1])\n",
    "                location[id_to_name[c]] = xyz\n",
    "            df = dict_to_df(location, run.name)\n",
    "            location_df.append(df)\n",
    "        location_df = pd.concat(location_df)\n",
    "        location_df.insert(loc=0, column='id', value=np.arange(len(location_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>particle_type</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>4552.042678</td>\n",
       "      <td>175.278626</td>\n",
       "      <td>221.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>3592.242408</td>\n",
       "      <td>324.003760</td>\n",
       "      <td>219.067449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>5627.594689</td>\n",
       "      <td>4866.047784</td>\n",
       "      <td>267.392371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>2055.706996</td>\n",
       "      <td>503.311447</td>\n",
       "      <td>409.365607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TS_5_4</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>2162.278628</td>\n",
       "      <td>3096.197053</td>\n",
       "      <td>435.081619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>343</td>\n",
       "      <td>TS_99_9</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>4481.115315</td>\n",
       "      <td>2714.801127</td>\n",
       "      <td>1105.844673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>344</td>\n",
       "      <td>TS_99_9</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>4366.031959</td>\n",
       "      <td>4131.991443</td>\n",
       "      <td>1104.551584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>345</td>\n",
       "      <td>TS_99_9</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>3744.348943</td>\n",
       "      <td>4572.760194</td>\n",
       "      <td>1092.200641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>346</td>\n",
       "      <td>TS_99_9</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>5193.082416</td>\n",
       "      <td>6274.865713</td>\n",
       "      <td>1083.910652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>347</td>\n",
       "      <td>TS_99_9</td>\n",
       "      <td>thyroglobulin</td>\n",
       "      <td>3626.887467</td>\n",
       "      <td>2136.230854</td>\n",
       "      <td>1102.047859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id experiment  particle_type            x            y            z\n",
       "0     0     TS_5_4  thyroglobulin  4552.042678   175.278626   221.391600\n",
       "1     1     TS_5_4  thyroglobulin  3592.242408   324.003760   219.067449\n",
       "2     2     TS_5_4  thyroglobulin  5627.594689  4866.047784   267.392371\n",
       "3     3     TS_5_4  thyroglobulin  2055.706996   503.311447   409.365607\n",
       "4     4     TS_5_4  thyroglobulin  2162.278628  3096.197053   435.081619\n",
       "..  ...        ...            ...          ...          ...          ...\n",
       "55  343    TS_99_9  thyroglobulin  4481.115315  2714.801127  1105.844673\n",
       "56  344    TS_99_9  thyroglobulin  4366.031959  4131.991443  1104.551584\n",
       "57  345    TS_99_9  thyroglobulin  3744.348943  4572.760194  1092.200641\n",
       "58  346    TS_99_9  thyroglobulin  5193.082416  6274.865713  1083.910652\n",
       "59  347    TS_99_9  thyroglobulin  3626.887467  2136.230854  1102.047859\n",
       "\n",
       "[348 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TS_5_4', 'TS_69_2', 'TS_6_4', 'TS_6_6', 'TS_73_6', 'TS_86_3', 'TS_99_9']\n",
      " TS_99_9 virus-like-particle\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(         particle_type    P    T  hit  miss   fp  precision    recall  \\\n",
       " 0         apo-ferritin    0  375    0   375    0   0.000000  0.000000   \n",
       " 1         beta-amylase    0   87    0    87    0   0.000000  0.000000   \n",
       " 2   beta-galactosidase    0  112    0   112    0   0.000000  0.000000   \n",
       " 3             ribosome    0  331    0   331    0   0.000000  0.000000   \n",
       " 4        thyroglobulin  348  251  173    78  175   0.497126  0.689243   \n",
       " 5  virus-like-particle    0  113    0   113    0   0.000000  0.000000   \n",
       " \n",
       "     f-beta4  weight  \n",
       " 0  0.000000       1  \n",
       " 1  0.000000       0  \n",
       " 2  0.000000       2  \n",
       " 3  0.000000       1  \n",
       " 4  0.673923       2  \n",
       " 5  0.000000       1  ,\n",
       " 0.19254943040460917)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helpers import *\n",
    "valid_dir = '../input/czii-cryo-et-object-identification/train'\n",
    "compute_lb(location_df, f'{valid_dir}/overlay/ExperimentRuns')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
